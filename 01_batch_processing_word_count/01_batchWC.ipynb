{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0c590f3-5673-4d38-8a5e-5b03de2b91e5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class batchWC():\n",
    "      def __init__(self):\n",
    "            self.base_data_dir=\"/FileStore/week01\"\n",
    "\n",
    "      def getRawData(self):\n",
    "            from pyspark.sql.functions import explode, split\n",
    "            lines= (spark.read\n",
    "                    .format(\"text\")\n",
    "                    .option(\"lineSep\", \".\")\n",
    "                    .load(f\"{self.base_data_dir}/data/\")\n",
    "                    )\n",
    "            return lines.select(explode(split(lines.value, \" \")).alias(\"word\"))\n",
    "      \n",
    "      def getQualityData(self, rawDF):\n",
    "            from pyspark.sql.functions import trim, lower\n",
    "            return (rawDF.select(lower(trim(rawDF.word)).alias(\"word\"))\n",
    "                    .where(\"word is not null\")\n",
    "                    .where(\"word rlike '[a-z]'\")\n",
    "                    )\n",
    "      def getWordCount(self, qualityDF):\n",
    "            return qualityDF.groupBy(\"word\").count()\n",
    "      \n",
    "      def overwriteWordCount(self, wordCountDF):\n",
    "            (wordCountDF.write\n",
    "             .format(\"delta\")\n",
    "             .mode(\"overwrite\")\n",
    "             .saveAsTable(\"word_count_table\")\n",
    "             )\n",
    "      \n",
    "      def wordCount(self):\n",
    "            print(f\"\\tExecuting Word Count...\", end='')\n",
    "            rawDF=self.getRawData()\n",
    "            qualityDF=self.getQualityData(rawDF)\n",
    "            resultDF = self.getWordCount(qualityDF)\n",
    "            self.overwriteWordCount(resultDF)\n",
    "            print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4a18f47-c973-4a69-83d8-d9a3f8345f10",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tExecuting Word Count...Done\n76665\n"
     ]
    }
   ],
   "source": [
    "bWC=batchWC()\n",
    "\n",
    "bWC.wordCount()\n",
    "t_cnt=spark.sql(\"select sum(count) from word_count_table\").collect()[0][0]\n",
    "print(t_cnt) #130853"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8726745-186c-4863-ad83-2b6354c2d293",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dbutils.fs.rm(\"/FileStore/week01\", True)\n",
    "dbutils.fs.mkdirs(\"/FileStore/week01/chekpoint\")\n",
    "#dbutils.fs.mkdirs(f\"{self.bases_data_dir}\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "01_batchWC",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
